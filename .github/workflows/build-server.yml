# Build llama-cpp-server, Adapted from https://github.com/ggerganov/llama.cpp/blob/master/.github/workflows/build.yml
name: Build server

on:
  workflow_dispatch: # allows manual triggering
    inputs:
      version:
        description: 'Version tag to build (e.g., v1.11.3)'
        required: false
        type: string
      create_release:
        description: 'Create release artifacts'
        required: false
        type: boolean
        default: false
  push:
    branches:
      - main
    paths: ['.github/workflows/build-server.yml', 'llama-cpp-server*/**']
  pull_request:
    paths: ['.github/workflows/build-server.yml', 'llama-cpp-server*/**']
  release:
    types: [published]

env:
  LLAMA_LOG_COLORS: 1
  LLAMA_LOG_PREFIX: 1
  LLAMA_LOG_TIMESTAMPS: 1
  LLAMA_LOG_VERBOSITY: 10

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build-multi-linux:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest]
        arch: [amd64, arm64]
        include:
          - arch: amd64
            platform: x86_64
          - arch: arm64
            platform: aarch64

    steps:
      - name: Clone
        uses: actions/checkout@v4
        with:
          submodules: "recursive"
          ref: ${{ github.event.inputs.version || github.ref }}
          fetch-depth: 0
          fetch-tags: true

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
        with:
          platforms: ${{ matrix.arch }}

      - name: Build
        run: |
          cat > build.sh << 'EOF'
          #!/bin/bash
          set -e
          apt-get update
          apt-get install -y build-essential libcurl4-openssl-dev cmake git
          cd /workspace/llama.cpp
          mkdir build
          cd build
          cmake .. -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CURL=OFF -DGGML_RPC=ON -DBUILD_SHARED_LIBS=OFF
          cmake --build . --config Release -j $(nproc)
          EOF
          chmod +x build.sh
          
          docker run --rm \
            --platform linux/${{ matrix.arch }} \
            -v ${{ github.workspace }}:/workspace \
            -v ${{ github.workspace }}/build.sh:/build.sh \
            ubuntu:latest \
            /build.sh

      - name: Test
        run: |
          docker run --rm \
            --platform linux/${{ matrix.arch }} \
            -v ${{ github.workspace }}:/workspace \
            ubuntu:latest \
            bash -c "apt-get update && apt-get install -y libcurl4-openssl-dev libgomp1 curl && /workspace/llama.cpp/build/bin/llama-server --version"

      - name: Determine tag name
        id: tag
        shell: bash
        run: |
          BUILD_NUMBER="$(git rev-list --count HEAD)"
          SHORT_HASH="$(git rev-parse --short=7 HEAD)"
          if [[ "${{ env.BRANCH_NAME }}" == "main" ]]; then
            echo "name=b${BUILD_NUMBER}" >> $GITHUB_OUTPUT
          else
            SAFE_NAME=$(echo "${{ env.BRANCH_NAME }}" | tr '/' '-')
            echo "name=${SAFE_NAME}-b${BUILD_NUMBER}-${SHORT_HASH}" >> $GITHUB_OUTPUT
          fi

      - name: Pack artifacts
        if: ${{ ( github.event_name == 'push' && github.ref == 'refs/heads/main' ) || github.event.inputs.create_release == 'true' }}
        run: |
          cp LICENSE ./llama.cpp/build/bin/
          zip -r llama-${{ steps.tag.outputs.name }}-bin-ubuntu-${{ matrix.arch }}.zip ./llama.cpp/build/bin/*

      - name: Upload artifacts
        if: ${{ ( github.event_name == 'push' && github.ref == 'refs/heads/main' ) || github.event.inputs.create_release == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          path: llama-${{ steps.tag.outputs.name }}-bin-ubuntu-${{ matrix.arch }}.zip
          name: llama-bin-ubuntu-${{ matrix.arch }}.zip

      - name: Upload server binary
        if: ${{ ( github.event_name == 'push' && github.ref == 'refs/heads/main' ) || github.event.inputs.create_release == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          path: llama.cpp/build/bin/llama-server
          name: llama-server-${{ matrix.arch }}

  ubuntu-latest-cmake-cuda:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.create_release == 'true' || github.event_name == 'release' }}
    strategy:
      matrix:
        arch: [amd64, arm64]

    steps:
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
        with:
          platforms: linux/${{ matrix.arch }}

      - name: Clone
        uses: actions/checkout@v4
        with:
          submodules: "recursive"
          ref: ${{ github.event.inputs.version || github.ref }}
          fetch-depth: 0
          fetch-tags: true

      - name: Build with CUDA
        run: |
          chmod +x scripts/build.sh
          docker run --rm \
            --platform linux/${{ matrix.arch }} \
            -v ${{ github.workspace }}:/workspace \
            -v ${{ github.workspace }}/scripts/build.sh:/build.sh \
            nvidia/cuda:12.6.2-devel-ubuntu24.04 \
            /build.sh

      - name: Determine tag name
        id: tag
        shell: bash
        run: |
          BUILD_NUMBER="$(git rev-list --count HEAD)"
          SHORT_HASH="$(git rev-parse --short=7 HEAD)"
          if [[ "${{ env.BRANCH_NAME }}" == "main" ]]; then
            echo "name=b${BUILD_NUMBER}" >> $GITHUB_OUTPUT
          else
            SAFE_NAME=$(echo "${{ env.BRANCH_NAME }}" | tr '/' '-')
            echo "name=${SAFE_NAME}-b${BUILD_NUMBER}-${SHORT_HASH}" >> $GITHUB_OUTPUT
          fi

      - name: Pack artifacts
        if: ${{ ( github.event_name == 'push' && github.ref == 'refs/heads/main' ) || github.event.inputs.create_release == 'true' }}
        run: |
          cp LICENSE ./llama.cpp/build/bin/
          zip -r llama-${{ steps.tag.outputs.name }}-bin-ubuntu-${{ matrix.arch }}-cuda.zip ./llama.cpp/build/bin/*

      - name: Upload artifacts
        if: ${{ ( github.event_name == 'push' && github.ref == 'refs/heads/main' ) || github.event.inputs.create_release == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          path: llama-${{ steps.tag.outputs.name }}-bin-ubuntu-${{ matrix.arch }}-cuda.zip
          name: llama-bin-ubuntu-${{ matrix.arch }}-cuda.zip

      - name: Upload server binary
        if: ${{ ( github.event_name == 'push' && github.ref == 'refs/heads/main' ) || github.event.inputs.create_release == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          path: llama.cpp/build/bin/llama-server
          name: llama-server-cuda-${{ matrix.arch }}

  build-wheels:
    needs: [build-multi-linux, ubuntu-latest-cmake-cuda]
    if: |
      always() &&
      needs.build-multi-linux.result == 'success'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10.14", "3.11.8", "3.12.2"]
    
    steps:
      - name: Check CUDA build
        id: check_cuda
        run: |
          if [[ "${{ github.event.inputs.create_release }}" == "true" || "${{ github.event_name }}" == "release" ]]; then
            echo "build_cuda=true" >> $GITHUB_OUTPUT
          else
            echo "build_cuda=false" >> $GITHUB_OUTPUT
          fi

      - name: Clone
        uses: actions/checkout@v4
        with:
          submodules: "recursive"
          ref: ${{ github.event.inputs.version || github.ref }}
          fetch-depth: 0
          fetch-tags: true

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Rye
        uses: eifinger/setup-rye@v4
        with:
          enable-cache: true
          working-directory: ./
          github-token: ${{ secrets.GITHUB_TOKEN }}

      # Build CPU version
      - name: Download CPU server binary (amd64)
        uses: actions/download-artifact@v4
        with:
          name: llama-server-amd64
          path: llama-cpp-server-py/src/llama_cpp_server_py/

      - name: Build CPU wheel (amd64)
        run: |
          cd llama-cpp-server-py
          echo "${{ matrix.python-version }}" > .python-version
          rye pin "${{ matrix.python-version }}"
          rye sync --no-dev
          WHEEL_PLATFORM="manylinux_2_31_x86_64" rye build
          cd ..

      - name: Upload CPU wheel (amd64)
        uses: actions/upload-artifact@v4
        with:
          name: dist-wheels-llama-cpp-server-py-x86_64-py${{ matrix.python-version }}
          path: dist/*.whl

      - name: Download CPU server binary (arm64)
        uses: actions/download-artifact@v4
        with:
          name: llama-server-arm64
          path: llama-cpp-server-py/src/llama_cpp_server_py/

      - name: Build CPU wheel (arm64)
        run: |
          cd llama-cpp-server-py
          echo "${{ matrix.python-version }}" > .python-version
          rye pin "${{ matrix.python-version }}"
          rye sync --no-dev
          WHEEL_PLATFORM="manylinux_2_31_aarch64" rye build
          cd ..

      - name: Upload CPU wheel (arm64)
        uses: actions/upload-artifact@v4
        with:
          name: dist-wheels-llama-cpp-server-py-aarch64-py${{ matrix.python-version }}
          path: dist/*.whl

      # Build CUDA version if needed
      - name: Download CUDA server binary (amd64)
        if: steps.check_cuda.outputs.build_cuda == 'true'
        uses: actions/download-artifact@v4
        with:
          name: llama-server-cuda-amd64
          path: llama-cpp-server-py-cuda/src/llama_cpp_server_py_cuda/

      - name: Build CUDA wheel (amd64)
        if: steps.check_cuda.outputs.build_cuda == 'true'
        run: |
          cd llama-cpp-server-py-cuda
          echo "${{ matrix.python-version }}" > .python-version
          rye pin "${{ matrix.python-version }}"
          rye sync --no-dev
          WHEEL_PLATFORM="manylinux_2_31_x86_64" rye build
          cd ..

      - name: Upload CUDA wheel (amd64)
        if: steps.check_cuda.outputs.build_cuda == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: dist-wheels-llama-cpp-server-py-cuda-x86_64-py${{ matrix.python-version }}
          path: dist/*.whl

      - name: Download CUDA server binary (arm64)
        if: steps.check_cuda.outputs.build_cuda == 'true'
        uses: actions/download-artifact@v4
        with:
          name: llama-server-cuda-arm64
          path: llama-cpp-server-py-cuda/src/llama_cpp_server_py_cuda/

      - name: Build CUDA wheel (arm64)
        if: steps.check_cuda.outputs.build_cuda == 'true'
        run: |
          cd llama-cpp-server-py-cuda
          echo "${{ matrix.python-version }}" > .python-version
          rye pin "${{ matrix.python-version }}"
          rye sync --no-dev
          WHEEL_PLATFORM="manylinux_2_31_aarch64" rye build
          cd ..

      - name: Upload CUDA wheel (arm64)
        if: steps.check_cuda.outputs.build_cuda == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: dist-wheels-llama-cpp-server-py-cuda-aarch64-py${{ matrix.python-version }}
          path: dist/*.whl